import os
import sys
import requests
import json
import gradio as gr
import logging
from typing import List, Optional, Dict, Tuple, Any
from dataclasses import dataclass
from dotenv import load_dotenv
import base64

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
class Config:
    API_BASE_URL = os.getenv('OLLAMA_API_URL', 'http://localhost:11434')
    API_GENERATE_URL = f"{API_BASE_URL}/api/generate"
    API_CHAT_URL = f"{API_BASE_URL}/api/chat"  # Added chat endpoint
    GRADIO_SERVER_PORT = int(os.getenv('GRADIO_SERVER_PORT', '7888'))
    OCR_TIMEOUT = int(os.getenv('OCR_TIMEOUT', '60'))  # å›¾ç‰‡å¤„ç†è¶…æ—¶æ—¶é—´
    CHAT_TIMEOUT = int(os.getenv('CHAT_TIMEOUT', '30'))  # é—®ç­”è¶…æ—¶æ—¶é—´
    MAX_HISTORY = int(os.getenv('MAX_HISTORY', '100'))
    RETRY_ATTEMPTS = int(os.getenv('RETRY_ATTEMPTS', '3'))

def check_ollama_service():
    try:
        requests.get(Config.API_BASE_URL, timeout=5)
        return True
    except:
        return False

@dataclass
class Conversation:
    history: List[str]
    max_length: int = Config.MAX_HISTORY

    def add_message(self, message: str) -> None:
        self.history.append(message)
        if len(self.history) > self.max_length:
            self.history = self.history[-self.max_length:]

    def get_context(self) -> str:
        return "\n".join(self.history)

    def clear(self) -> None:
        self.history = []

class ChatBot:
    """
    æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿçš„æ ¸å¿ƒç±»ï¼Œæä¾›OCRå’Œå¤šè½®å¯¹è¯åŠŸèƒ½ã€‚
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. çµæ´»çš„å›¾åƒè¯†åˆ«ï¼šæ”¯æŒè‡ªå®šä¹‰OCRè¯†åˆ«æç¤ºï¼Œå¯ä»¥é’ˆå¯¹ä¸åŒç±»å‹çš„æ–‡æ¡£ä¼˜åŒ–è¯†åˆ«æ•ˆæœ
    2. å¤šè½®å¯¹è¯ï¼šæ”¯æŒåœ¨å¤šä¸ªç‹¬ç«‹çš„å¯¹è¯ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œäº¤äº’
    3. é”™è¯¯å¤„ç†ï¼šæä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œæ•…éšœæ’é™¤å»ºè®®
    
    å±æ€§ï¼š
        conversation: ä¸»å¯¹è¯å†å²è®°å½•
        qa_conversations: å¤šä¸ªç‹¬ç«‹çš„é—®ç­”å¯¹è¯å†å²è®°å½•
        session: HTTPä¼šè¯å¯¹è±¡
        current_image: å½“å‰å¤„ç†çš„å›¾ç‰‡
        current_ocr_result: å½“å‰çš„OCRè¯†åˆ«ç»“æœ
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ChatBotå®ä¾‹ï¼Œè®¾ç½®å¯¹è¯å†å²å’Œä¼šè¯å¯¹è±¡"""
        self.conversation = Conversation([])
        self.session = requests.Session()
        self.current_image: Optional[str] = None
        self.current_ocr_result: Optional[str] = None
        # Add separate conversation histories for each QA section
        self.qa_conversations = {
            "qa1": Conversation([]),
            "qa2": Conversation([]),
            "qa3": Conversation([])
        }

    def _encode_image(self, image_path: str) -> str:
        # Check file size before processing
        file_size = os.path.getsize(image_path)
        max_size = 10 * 1024 * 1024  # 10MB limit
        if file_size > max_size:
            raise ValueError(f"Image file too large ({file_size / 1024 / 1024:.1f}MB). Maximum size is 10MB.")
        
        # Check file extension
        valid_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.webp'}
        if not os.path.splitext(image_path)[1].lower() in valid_extensions:
            raise ValueError("Invalid image format. Supported formats: JPG, PNG, GIF, WEBP")
            
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    def generate_response(self, prompt: str, model_name: str, image: Optional[str] = None, is_ocr: bool = False, conversation_id: Optional[str] = None) -> str:
        """
        ç”Ÿæˆå“åº”ï¼Œæ”¯æŒOCRå’Œé—®ç­”ä¸¤ç§æ¨¡å¼ï¼Œå¹¶æ”¯æŒå¤šè½®å¯¹è¯
        :param prompt: æç¤ºè¯
        :param model_name: æ¨¡å‹åç§°
        :param image: å¯é€‰çš„å›¾ç‰‡æ•°æ®
        :param is_ocr: æ˜¯å¦æ˜¯OCRæ¨¡å¼
        :param conversation_id: å¯¹è¯IDï¼Œç”¨äºå¤šè½®å¯¹è¯
        """
        for attempt in range(Config.RETRY_ATTEMPTS):
            try:
                timeout = Config.OCR_TIMEOUT if is_ocr else Config.CHAT_TIMEOUT
                
                # è·å–å¯¹è¯å†å²
                conversation = self.qa_conversations.get(conversation_id, self.conversation) if conversation_id else self.conversation
                context = conversation.get_context()
                
                # æ„å»ºå®Œæ•´çš„æç¤ºè¯ï¼ŒåŒ…å«å†å²å¯¹è¯
                full_prompt = f"{context}\n\n{prompt}" if context else prompt
                
                # æ·»åŠ OCRç‰¹å®šçš„é”™è¯¯å¤„ç†
                if is_ocr and image:
                    try:
                        # éªŒè¯å›¾ç‰‡æ•°æ®
                        if not image or len(image) < 100:  # åŸºæœ¬éªŒè¯
                            return "é”™è¯¯: å›¾ç‰‡æ•°æ®æ— æ•ˆæˆ–æŸå"
                    except Exception as e:
                        return f"é”™è¯¯: å›¾ç‰‡å¤„ç†å¤±è´¥ - {str(e)}"
                
                headers = {"Content-Type": "application/json"}
                payload = {
                    "model": model_name.replace("ğŸ–¼ï¸ ", ""),
                    "prompt": full_prompt,
                    "stream": True
                }
                
                if image:
                    payload["images"] = [image]
                
                try:
                    response = self.session.post(
                        Config.API_GENERATE_URL,
                        headers=headers,
                        json=payload,
                        stream=True,
                        timeout=timeout
                    )
                except requests.exceptions.Timeout:
                    return "é”™è¯¯: æœåŠ¡å™¨å“åº”è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•"
                except requests.exceptions.ConnectionError:
                    return "é”™è¯¯: æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
                
                if response.status_code != 200:
                    error_msg = "æœåŠ¡å™¨é”™è¯¯"
                    try:
                        error_data = response.json()
                        if 'error' in error_data:
                            error_msg = error_data['error']
                    except:
                        if response.text:
                            error_msg = response.text
                    # æä¾›æ›´å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
                    if "model not found" in error_msg.lower():
                        return f"é”™è¯¯: æ¨¡å‹ {model_name} æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿å·²å®‰è£…è¯¥æ¨¡å‹"
                    elif "out of memory" in error_msg.lower():
                        return "é”™è¯¯: æœåŠ¡å™¨å†…å­˜ä¸è¶³ï¼Œè¯·ç¨åé‡è¯•æˆ–å°è¯•å¤„ç†è¾ƒå°çš„å›¾ç‰‡"
                    else:
                        return f"é”™è¯¯: {error_msg}"
                
                full_response = []
                try:
                    for line in response.iter_lines(decode_unicode=True):
                        if line:
                            try:
                                data = json.loads(line)
                                if "response" in data:
                                    full_response.append(data["response"])
                            except json.JSONDecodeError:
                                continue
                except Exception as e:
                    return f"é”™è¯¯: å¤„ç†æœåŠ¡å™¨å“åº”æ—¶å‡ºé”™ - {str(e)}"
                
                if full_response:
                    final_response = "".join(full_response)
                    # å°†å¯¹è¯è®°å½•æ·»åŠ åˆ°ç›¸åº”çš„å†å²è®°å½•ä¸­
                    if conversation_id:
                        self.qa_conversations[conversation_id].add_message(f"User: {prompt}")
                        self.qa_conversations[conversation_id].add_message(f"Assistant: {final_response}")
                    else:
                        self.conversation.add_message(final_response)
                    return final_response
                else:
                    return "é”™è¯¯: æœåŠ¡å™¨è¿”å›äº†ç©ºå“åº”"
                    
            except Exception as e:
                logger.error(f"Request attempt {attempt + 1} failed: {str(e)}")
                if attempt == Config.RETRY_ATTEMPTS - 1:
                    return f"é”™è¯¯: å¤šæ¬¡å°è¯•åä»ç„¶å¤±è´¥ - {str(e)}"
                continue
            
            except Exception as e:
                logger.error(f"Unexpected error: {str(e)}")
                return f"Error: An unexpected error occurred: {str(e)}"

    def get_model_list(self) -> Tuple[List[str], str]:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        
        Returns:
            Tuple[List[str], str]: è¿”å›ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«æ¨¡å‹åˆ—è¡¨å’ŒçŠ¶æ€ä¿¡æ¯
        """
        try:
            response = self.session.get(f"{Config.API_BASE_URL}/api/tags", timeout=Config.CHAT_TIMEOUT)
            response.raise_for_status()
            data = response.json()
            
            if 'models' in data:
                # Extract model names 
                models = [model.get('name', '') for model in data['models']]
                return models, "æˆåŠŸè·å–æ¨¡å‹åˆ—è¡¨"
            else:
                return [], "APIè¿”å›æ•°æ®æ ¼å¼ä¸æ­£ç¡®"
                
        except requests.exceptions.RequestException as e:
            logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
            return [], f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}"
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
            return [], f"è·å–æ¨¡å‹åˆ—è¡¨æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}"

    def clear_conversation(self, conversation_id: Optional[str] = None) -> str:
        """æ¸…ç©ºæŒ‡å®šå¯¹è¯å†å²æˆ–æ‰€æœ‰å¯¹è¯å†å²"""
        if conversation_id:
            if conversation_id in self.qa_conversations:
                self.qa_conversations[conversation_id].clear()
                return f"Conversation {conversation_id} cleared."
        else:
            self.conversation.clear()
            for conv in self.qa_conversations.values():
                conv.clear()
            return "All conversations cleared."

def create_interface() -> None:
    try:
        # é¦–å…ˆæ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨
        if not check_ollama_service():
            logger.error("æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡")
            with gr.Blocks() as interface:
                gr.Markdown("# æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿ - æœåŠ¡æœªè¿æ¥")
                gr.Markdown("""
                ### âš ï¸ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡
                
                è¯·æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
                1. ç¡®ä¿OllamaæœåŠ¡å·²ç»å¯åŠ¨
                2. æ£€æŸ¥ç¯å¢ƒå˜é‡OLLAMA_API_URLæ˜¯å¦æ­£ç¡®è®¾ç½®
                3. é»˜è®¤åœ°å€æ˜¯http://localhost:11434ï¼Œç¡®ä¿æ­¤ç«¯å£å¯è®¿é—®
                
                ç³»ç»Ÿä¼šåœ¨æœåŠ¡å¯ç”¨æ—¶è‡ªåŠ¨é‡æ–°è¿æ¥ã€‚
                """)
                retry_button = gr.Button("é‡è¯•è¿æ¥")
                status_text = gr.Textbox(
                    label="çŠ¶æ€",
                    value="ç­‰å¾…æœåŠ¡è¿æ¥...",
                    interactive=False
                )
                
                retry_button.click(
                    lambda: "æœåŠ¡å·²è¿æ¥ï¼Œè¯·åˆ·æ–°é¡µé¢" if check_ollama_service() else "æœåŠ¡ä»ç„¶æ— æ³•è¿æ¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€",
                    None,
                    status_text
                )
            return
        
        chatbot = ChatBot()
        
        # é¢„å…ˆè·å–æ¨¡å‹åˆ—è¡¨
        models, status = chatbot.get_model_list()
        if not models:
            logger.warning(f"æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨: {status}")
            with gr.Blocks() as interface:
                gr.Markdown("# æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿ - æ— å¯ç”¨æ¨¡å‹")
                gr.Markdown(f"""
                ### âš ï¸ æœªæ£€æµ‹åˆ°å¯ç”¨æ¨¡å‹
                
                è¯·ç¡®ä¿å®‰è£…äº†æ‰€éœ€çš„æ¨¡å‹ï¼š
                1. è¿è¡Œ `ollama pull llava` å®‰è£…llavaæ¨¡å‹
                2. æˆ–è¿è¡Œ `ollama pull bakllava` å®‰è£…bakllavaæ¨¡å‹
                3. æˆ–å®‰è£…å…¶ä»–æ”¯æŒå›¾åƒåˆ†æçš„æ¨¡å‹
                
                é”™è¯¯ä¿¡æ¯ï¼š{status}
                """)
                retry_button = gr.Button("é‡æ–°æ£€æŸ¥æ¨¡å‹")
                status_text = gr.Textbox(
                    label="çŠ¶æ€",
                    value="ç­‰å¾…å®‰è£…æ¨¡å‹...",
                    interactive=False
                )
                
                retry_button.click(
                    lambda: "æ£€æµ‹åˆ°æ¨¡å‹ï¼Œè¯·åˆ·æ–°é¡µé¢" if chatbot.get_model_list()[0] else f"ä»æœªæ£€æµ‹åˆ°æ¨¡å‹: {status}",
                    None,
                    status_text
                )
            return
            
        # 
        all_models = models
        
        with gr.Blocks(css="""
            :root {
                --primary-color: #2563eb;
                --primary-light: #3b82f6;
                --success-color: #059669;
                --warning-color: #d97706;
                --error-color: #dc2626;
                --background-color: #f3f4f6;
                --card-background: #ffffff;
                --text-primary: #1f2937;
                --text-secondary: #4b5563;
                --border-color: #e5e7eb;
                --chat-user-bg: #e5e7eb;
                --chat-assistant-bg: #dbeafe;
            }
            
            .container { 
                max-width: 800px; 
                margin: 0 auto; 
                padding: 0.5rem;
                background-color: var(--background-color);
                min-height: auto;
                height: auto;
                display: flex;
                flex-direction: column;
            }
            
            .header {
                text-align: center;
                margin-bottom: 0.75rem;
                color: var(--text-primary);
                flex: none;
            }
            
            .section {
                background-color: var(--card-background);
                border: 1px solid var(--border-color);
                border-radius: 8px;
                padding: 1rem;
                margin-bottom: 1rem;
                box-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
                height: auto;
                min-height: 0;
                display: flex;
                flex-direction: column;
            }
            
            .gr-image {
                border-radius: 6px !important;
                overflow: hidden !important;
                transition: all 0.2s !important;
                border: 2px solid transparent !important;
                height: auto !important;
                max-height: 400px !important;
                object-fit: contain !important;
            }
            
            .gr-form {
                gap: 0.75rem !important;
                height: auto !important;
                min-height: 0 !important;
            }
            
            .image-container {
                display: flex;
                height: auto;
                flex-direction: column;
                justify-content: flex-start;
                align-items: center;
                max-height: none;
                min-height: 0;
            }
            
            .step-label {
                margin: 0.5rem 0;
                color: var(--text-secondary);
                font-size: 0.9rem;
            }
            
            .result-label {
                margin: 0.5rem 0;
                color: var(--text-secondary);
                font-size: 0.9rem;
            }
            
            .gr-form {
                gap: 0.5rem !important;
            }
            
            .gr-input, .gr-dropdown {
                margin-bottom: 0.5rem !important;
            }
            
            .chat-history-container {
                max-height: 300px;
                overflow-y: auto;
                border: 1px solid var(--border-color);
                border-radius: 8px;
                margin-bottom: 1rem;
                padding: 0.5rem;
                background-color: var(--card-background);
            }
            
            .user-message {
                background-color: var(--chat-user-bg);
                padding: 0.5rem;
                margin: 0.25rem 0;
                border-radius: 8px;
                max-width: 85%;
                margin-left: auto;
                word-wrap: break-word;
            }
            
            .assistant-message {
                background-color: var(--chat-assistant-bg);
                padding: 0.5rem;
                margin: 0.25rem 0;
                border-radius: 8px;
                max-width: 85%;
                margin-right: auto;
                word-wrap: break-word;
            }
        """) as interface:
            with gr.Column(elem_classes="container"):
                with gr.Column(elem_classes="header"):
                    gr.Markdown("# æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿ")
                
                # å›¾åƒåˆ†æåŒºåŸŸ
                with gr.Column():
                    gr.Markdown("### å›¾åƒåˆ†æ")
                    image_input = gr.Image(
                        type="filepath",
                        label="ä¸Šä¼ å›¾ç‰‡"
                    )
                    
                    ocr_question_input = gr.Textbox(
                        label="å›¾åƒåˆ†æé—®é¢˜",
                        placeholder="è¯·è¾“å…¥å…³äºå›¾ç‰‡çš„å…·ä½“é—®é¢˜ï¼Œä¾‹å¦‚ï¼š'è¯·åˆ†æå›¾ç‰‡ä¸­çš„å†…å®¹' æˆ– 'è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—'",
                        lines=2
                    )
                    
                    with gr.Row():
                        ocr_model_selector = gr.Dropdown(
                            choices=all_models,
                            label="é€‰æ‹©å›¾åƒåˆ†ææ¨¡å‹",
                            value=all_models[0] if all_models else None
                        )
                        ocr_retry_button = gr.Button(
                            "é‡æ–°åˆ†æ",
                            variant="secondary",
                            elem_classes="secondary"
                        )
                    ocr_output = gr.Textbox(
                        label="åˆ†æç»“æœ",
                        lines=4,
                        interactive=False,
                        elem_classes="output-text"
                    )
                
                # é—®ç­”åŒºåŸŸ1
                with gr.Column():
                    gr.Markdown("### é—®ç­”1")
                    with gr.Column(elem_classes="chat-history"):
                        chat_history1 = gr.HTML(label="å¯¹è¯å†å²")
                    question_input1 = gr.Textbox(
                        label="è¾“å…¥é—®é¢˜",
                        lines=2,
                        interactive=True
                    )
                    with gr.Row():
                        chat_model_selector1 = gr.Dropdown(
                            choices=all_models,
                            label="é€‰æ‹©é—®ç­”æ¨¡å‹",
                            value=all_models[0] if all_models else None
                        )
                        qa_retry_button1 = gr.Button(
                            "é‡æ–°å›ç­”",
                            variant="secondary",
                            elem_classes="secondary"
                        )
                    chat_output1 = gr.Textbox(
                        label="å›ç­”",
                        lines=4,
                        interactive=False,
                        elem_classes="output-text"
                    )

                # é—®ç­”åŒºåŸŸ2
                with gr.Column():
                    gr.Markdown("### é—®ç­”2")
                    with gr.Column(elem_classes="chat-history"):
                        chat_history2 = gr.HTML(label="å¯¹è¯å†å²")
                    question_input2 = gr.Textbox(
                        label="è¾“å…¥é—®é¢˜",
                        lines=2,
                        interactive=True
                    )
                    with gr.Row():
                        chat_model_selector2 = gr.Dropdown(
                            choices=all_models,
                            label="é€‰æ‹©é—®ç­”æ¨¡å‹",
                            value=all_models[0] if all_models else None
                        )
                        qa_retry_button2 = gr.Button(
                            "é‡æ–°å›ç­”",
                            variant="secondary",
                            elem_classes="secondary"
                        )
                    chat_output2 = gr.Textbox(
                        label="å›ç­”",
                        lines=4,
                        interactive=False,
                        elem_classes="output-text"
                    )

                # é—®ç­”åŒºåŸŸ3
                with gr.Column():
                    gr.Markdown("### é—®ç­”3")
                    with gr.Column(elem_classes="chat-history"):
                        chat_history3 = gr.HTML(label="å¯¹è¯å†å²")
                    question_input3 = gr.Textbox(
                        label="è¾“å…¥é—®é¢˜",
                        lines=2,
                        interactive=True
                    )
                    with gr.Row():
                        chat_model_selector3 = gr.Dropdown(
                            choices=all_models,
                            label="é€‰æ‹©é—®ç­”æ¨¡å‹",
                            value=all_models[0] if all_models else None
                        )
                        qa_retry_button3 = gr.Button(
                            "é‡æ–°å›ç­”",
                            variant="secondary",
                            elem_classes="secondary"
                        )
                    chat_output3 = gr.Textbox(
                        label="å›ç­”",
                        lines=4,
                        interactive=False,
                        elem_classes="output-text"
                    )
                
                # ä¸»æ§åˆ¶åŒº
                with gr.Row():
                    process_button = gr.Button(
                        "å¼€å§‹å¤„ç†",
                        variant="primary",
                        elem_classes="primary"
                    )
                    clear_button = gr.Button(
                        "æ¸…ç©º",
                        variant="secondary",
                        elem_classes="secondary"
                    )
                
                # çŠ¶æ€æ˜¾ç¤º
                with gr.Row():
                    status_text = gr.Textbox(
                        label="çŠ¶æ€",
                        interactive=False,
                        value="è¯·ä¸Šä¼ å›¾ç‰‡å¹¶è¾“å…¥é—®é¢˜",
                        elem_classes="status"
                    )
                
                # éšè—çš„OCRç»“æœå­˜å‚¨
                ocr_result_store = gr.State(value=None)

            # å¤„ç†å‡½æ•°ï¼šä¸€é”®å¤„ç†æ‰€æœ‰æµç¨‹
            def process_all(image_path, ocr_question, ocr_model, question1, chat_model1, 
                          question2, chat_model2, question3, chat_model3):
                """ä¸€é”®å¤„ç†æ‰€æœ‰æµç¨‹"""
                if not image_path or not ocr_model:
                    return {
                        ocr_output: "è¯·ä¸Šä¼ å›¾ç‰‡å¹¶é€‰æ‹©å›¾åƒåˆ†ææ¨¡å‹",
                        chat_output1: "",
                        chat_output2: "",
                        chat_output3: "",
                        status_text: "ç­‰å¾…è¾“å…¥",
                        ocr_result_store: None,
                        chat_history1: "",
                        chat_history2: "",
                        chat_history3: ""
                    }
                    
                try:
                    chatbot = ChatBot()
                    
                    # ç¬¬ä¸€æ­¥ï¼šå›¾ç‰‡åˆ†æ
                    base64_image = chatbot._encode_image(image_path)
                    analysis_prompt = ocr_question.strip() if ocr_question.strip() else "è¯·åˆ†æå¹¶æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"
                    analysis_result = chatbot.generate_response(analysis_prompt, ocr_model, base64_image, is_ocr=True)
                    
                    # ç¬¬äºŒæ­¥ï¼šé—®ç­”å¤„ç†
                    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰é—®ç­”æ¡†éƒ½æœ‰å†…å®¹
                    has_question1 = bool(question1.strip())
                    has_question2 = bool(question2.strip())
                    has_question3 = bool(question3.strip())
                    
                    # å¦‚æœç¬¬ä¸€ä¸ªé—®ç­”æ¡†ä¸ºç©ºï¼Œç›´æ¥è¿”å›
                    if not has_question1:
                        return {
                            ocr_output: analysis_result,
                            chat_output1: "è¯·åœ¨é—®ç­”æ¡†1ä¸­è¾“å…¥é—®é¢˜",
                            chat_output2: "",
                            chat_output3: "",
                            status_text: "ç­‰å¾…é—®é¢˜è¾“å…¥",
                            ocr_result_store: analysis_result,
                            chat_history1: "",
                            chat_history2: "",
                            chat_history3: ""
                        }
                    
                    # å¤„ç†é—®ç­”1
                    chat_output1_text = ""
                    chat_history1_html = ""
                    if chat_model1:
                        qa_prompt1 = f"åŸºäºä»¥ä¸‹åˆ†æç»“æœå›ç­”é—®é¢˜ã€‚\nåˆ†æç»“æœï¼š\n{analysis_result}\n\né—®é¢˜ï¼š{question1}"
                        chat_output1_text = chatbot.generate_response(qa_prompt1, chat_model1, conversation_id="qa1")
                        chat_history1_html = format_chat_history(chatbot.qa_conversations["qa1"].history)
                    
                    # å¤„ç†é—®ç­”2
                    chat_output2_text = ""
                    chat_history2_html = ""
                    if chat_model2:
                        # å¦‚æœé—®ç­”æ¡†2æœ‰å†…å®¹ï¼Œä½¿ç”¨é—®ç­”æ¡†2çš„é—®é¢˜ï¼Œå¦åˆ™ä½¿ç”¨é—®ç­”æ¡†1çš„é—®é¢˜
                        qa_question2 = question2.strip() if has_question2 else question1.strip()
                        qa_prompt2 = f"åŸºäºä»¥ä¸‹åˆ†æç»“æœå›ç­”é—®é¢˜ã€‚\nåˆ†æç»“æœï¼š\n{analysis_result}\n\né—®é¢˜ï¼š{qa_question2}"
                        chat_output2_text = chatbot.generate_response(qa_prompt2, chat_model2, conversation_id="qa2")
                        chat_history2_html = format_chat_history(chatbot.qa_conversations["qa2"].history)
                    
                    # å¤„ç†é—®ç­”3
                    chat_output3_text = ""
                    chat_history3_html = ""
                    if chat_model3:
                        # å¦‚æœé—®ç­”æ¡†3æœ‰å†…å®¹ï¼Œä½¿ç”¨é—®ç­”æ¡†3çš„é—®é¢˜ï¼Œå¦åˆ™ä½¿ç”¨é—®ç­”æ¡†1çš„é—®é¢˜
                        qa_question3 = question3.strip() if has_question3 else question1.strip()
                        qa_prompt3 = f"åŸºäºä»¥ä¸‹åˆ†æç»“æœå›ç­”é—®é¢˜ã€‚\nåˆ†æç»“æœï¼š\n{analysis_result}\n\né—®é¢˜ï¼š{qa_question3}"
                        chat_output3_text = chatbot.generate_response(qa_prompt3, chat_model3, conversation_id="qa3")
                        chat_history3_html = format_chat_history(chatbot.qa_conversations["qa3"].history)
                    
                    return {
                        ocr_output: analysis_result,
                        chat_output1: chat_output1_text,
                        chat_output2: chat_output2_text,
                        chat_output3: chat_output3_text,
                        status_text: "å¤„ç†å®Œæˆ",
                        ocr_result_store: analysis_result,
                        chat_history1: chat_history1_html,
                        chat_history2: chat_history2_html,
                        chat_history3: chat_history3_html
                    }
                    
                except Exception as e:
                    logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                    return {
                        ocr_output: f"é”™è¯¯: {str(e)}",
                        chat_output1: "",
                        chat_output2: "",
                        chat_output3: "",
                        status_text: "å¤„ç†å‡ºé”™",
                        ocr_result_store: None,
                        chat_history1: "",
                        chat_history2: "",
                        chat_history3: ""
                    }

            # æ ¼å¼åŒ–å¯¹è¯å†å²ä¸ºHTML
            def format_chat_history(history: List[str]) -> str:
                if not history:
                    return ""
                html = "<div class='chat-history-container'>"
                for msg in history:
                    if msg.startswith("User: "):
                        html += f"<div class='user-message'>{msg[6:]}</div>"
                    elif msg.startswith("Assistant: "):
                        html += f"<div class='assistant-message'>{msg[11:]}</div>"
                html += "</div>"
                return html

            # ç»‘å®šäº‹ä»¶å¤„ç†å‡½æ•°
            process_button.click(
                process_all,
                [image_input, ocr_question_input, ocr_model_selector,
                 question_input1, chat_model_selector1,
                 question_input2, chat_model_selector2,
                 question_input3, chat_model_selector3],
                [ocr_output, chat_output1, chat_output2, chat_output3,
                 status_text, ocr_result_store,
                 chat_history1, chat_history2, chat_history3]
            )

            # æ¸…ç©ºæŒ‰é’®äº‹ä»¶
            def clear_all():
                return [""] * 9  # è¿”å›9ä¸ªç©ºå­—ç¬¦ä¸²

            clear_button.click(
                clear_all,
                None,
                [ocr_output, chat_output1, chat_output2, chat_output3,
                 status_text, ocr_result_store,
                 chat_history1, chat_history2, chat_history3]
            )

            # é‡è¯•æŒ‰é’®äº‹ä»¶
            ocr_retry_button.click(
                process_all,
                [image_input, ocr_question_input, ocr_model_selector,
                 question_input1, chat_model_selector1,
                 question_input2, chat_model_selector2,
                 question_input3, chat_model_selector3],
                [ocr_output, chat_output1, chat_output2, chat_output3,
                 status_text, ocr_result_store,
                 chat_history1, chat_history2, chat_history3]
            )
        
        interface.launch(
            server_port=Config.GRADIO_SERVER_PORT,
            share=True,
            inbrowser=True  # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
        )
        
    except Exception as e:
        logger.error(f"Failed to create interface: {e}")
        sys.exit(1)

if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—è¾“å‡ºåˆ°æ–‡ä»¶
    log_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'chatbot.log')
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    logger.addHandler(file_handler)
    
    try:
        logger.info("Starting chatbot interface...")
        print("æ­£åœ¨å¯åŠ¨èŠå¤©æœºå™¨äººç•Œé¢...")
        if not check_ollama_service():
            error_msg = "Error: Ollama service is not running. Please start Ollama first."
            logger.error(error_msg)
            print(error_msg)
            input("æŒ‰å›è½¦é”®é€€å‡º...")
            sys.exit(1)
            
        demo = create_interface()
        demo.launch(
            server_name="0.0.0.0",
            server_port=Config.GRADIO_SERVER_PORT,
            share=False,
            inbrowser=True
        )
    except Exception as e:
        print(f"\nError: {str(e)}")
        logger.error(f"Application error: {str(e)}", exc_info=True)
        print("\nPress Enter to exit...")
        input()
        sys.exit(1)
